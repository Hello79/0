{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0927_seoyeon_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJPo0JTyuFEJ"
      },
      "source": [
        "# 4. word embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oErzBrZPwVZB"
      },
      "source": [
        "max_len = 100\n",
        "tokenizer = Tokenizer()\n",
        "vocabulary = tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kO9MjegBuUh9"
      },
      "source": [
        "## (1) Keras Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJHjlyW6uHgK"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 128, input_length = max_len))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D4jhtsBudBH"
      },
      "source": [
        "## (2) word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2_p2lMMugE4"
      },
      "source": [
        "import gensim\n",
        "word2vec = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/ESAA-2/GoogleNews-vectors-negative300.bin.gz', binary = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99cee28OwOLX"
      },
      "source": [
        "embedding_matrix = np.zeros((vocab_size, 300)) #300차원의 임베딩 매트릭스 생성\n",
        "\n",
        "for index, word in enumerate(vocabulary): #vocabulary에 있는 토큰들을 하나씩 넘겨줍니다.\n",
        "    if word in word2vec: #넘겨 받은 토큰이 word2vec에 존재하면(이미 훈련이 된 토큰이라는 뜻)\n",
        "        embedding_vector = word2vec[word] #해당 토큰에 해당하는 vector를 불러오고\n",
        "        embedding_mxtrix[i] = embedding_vector #해당 위치의 embedding_mxtrix에 저장합니다.\n",
        "    else:\n",
        "        print(\"word2vec에 없는 단어입니다.\")\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIpLmXjLwRvP"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 300,weights = [embedding_matrix], input_length = max_len))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF3DaZ1Jwo2l"
      },
      "source": [
        "## (3) glove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa9x0_FSwqgT"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# load the whole embedding into memory\n",
        "glove = dict()\n",
        "f = open('/content/drive/MyDrive/ESAA-2/glove.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    vector = np.asarray(values[1:], dtype='float32')\n",
        "    glove[word] = vector\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFJ69_VnwvjT"
      },
      "source": [
        "embedding_matrix = np.zeros((vocab_size, 300)) #300차원의 임베딩 매트릭스 생성\n",
        "\n",
        "for index, word in enumerate(vocabulary): #vocabulary에 있는 토큰들을 하나씩 넘겨줍니다.\n",
        "    if word in glove: #넘겨 받은 토큰이 word2vec에 존재하면(이미 훈련이 된 토큰이라는 뜻)\n",
        "        embedding_vector = glove[word] #해당 토큰에 해당하는 vector를 불러오고\n",
        "        embedding_mxtrix[i] = embedding_vector #해당 위치의 embedding_mxtrix에 저장합니다.\n",
        "    else:\n",
        "        print(\"glove 없는 단어입니다.\")\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqgTZG2hw2Jo"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 300,weights = [embedding_matrix], input_length = max_len))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw0EOlqPzXw4"
      },
      "source": [
        "## (4) Fasttext"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv1sVp0nzar5"
      },
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "FastText = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/ESAA-2/fasttext.vec', binary = True, unicode_errors='ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDsfazYx0Z6A"
      },
      "source": [
        "embedding_matrix = np.zeros((vocab_size, 300)) #300차원의 임베딩 매트릭스 생성\n",
        "\n",
        "for index, word in enumerate(vocabulary): #vocabulary에 있는 토큰들을 하나씩 넘겨줍니다.\n",
        "    if word in word2vec: #넘겨 받은 토큰이 word2vec에 존재하면(이미 훈련이 된 토큰이라는 뜻)\n",
        "        embedding_vector = word2vec[word] #해당 토큰에 해당하는 vector를 불러오고\n",
        "        embedding_mxtrix[i] = embedding_vector #해당 위치의 embedding_mxtrix에 저장합니다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsEqA7TL0eGx"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 300,weights = [embedding_matrix], input_length = max_len))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWcn3nfW1NEM"
      },
      "source": [
        "# 5. Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "096Enm6g5ncp"
      },
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/ESAA-2/0913_train.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sra4MUOf1Vjb"
      },
      "source": [
        "## (1) 간단한 전처리 + 형태소 분석"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7vHTOyu6rFj",
        "outputId": "b97c2b60-5b97-436f-ad84-004fce549669"
      },
      "source": [
        "from konlpy.tag import Okt\n",
        "import re\n",
        "import tqdm \n",
        "\n",
        "def text_preprocessing(text_list):\n",
        "    \n",
        "    stopwords = ['을', '를', '이', '가', '은', '는', 'null'] #불용어 설정\n",
        "    tokenizer = Okt() #형태소 분석기 \n",
        "    token_list = []\n",
        "    \n",
        "    for text in tqdm.tqdm(text_list):\n",
        "        txt = re.sub('[^가-힣a-z]', ' ', text) #한글과 영어 소문자만 남기고 다른 글자 모두 제거\n",
        "        token = tokenizer.morphs(txt) #형태소 분석\n",
        "        token = [t for t in token if t not in stopwords or type(t) != float] #형태소 분석 결과 중 stopwords에 해당하지 않는 것만 추출\n",
        "        token_list.append(token)\n",
        "        \n",
        "    return token_list, tokenizer\n",
        "\n",
        "#형태소 분석기를 따로 저장한 이유는 후에 test 데이터 전처리를 진행할 때 이용해야 되기 때문입니다. \n",
        "train['token'], okt = text_preprocessing(train['text'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 54879/54879 [01:05<00:00, 843.89it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "INIl9Jcl7sfg",
        "outputId": "20f238ba-4f59-411d-86c1-89422f51b419"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>text</th>\n",
              "      <th>author</th>\n",
              "      <th>token</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>He was almost choking. There was so much, so m...</td>\n",
              "      <td>3</td>\n",
              "      <td>[e, was, almost, choking, here, was, so, much,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>“Your sister asked for it, I suppose?”</td>\n",
              "      <td>2</td>\n",
              "      <td>[our, sister, asked, for, it, suppose]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>She was engaged one day as she walked, in per...</td>\n",
              "      <td>1</td>\n",
              "      <td>[he, was, engaged, one, day, as, she, walked, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>The captain was in the porch, keeping himself ...</td>\n",
              "      <td>4</td>\n",
              "      <td>[he, captain, was, in, the, porch, keeping, hi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>“Have mercy, gentlemen!” odin flung up his han...</td>\n",
              "      <td>3</td>\n",
              "      <td>[ave, mercy, gentlemen, odin, flung, up, his, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index  ...                                              token\n",
              "0      0  ...  [e, was, almost, choking, here, was, so, much,...\n",
              "1      1  ...             [our, sister, asked, for, it, suppose]\n",
              "2      2  ...  [he, was, engaged, one, day, as, she, walked, ...\n",
              "3      3  ...  [he, captain, was, in, the, porch, keeping, hi...\n",
              "4      4  ...  [ave, mercy, gentlemen, odin, flung, up, his, ...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxbi7rPL1Xbd"
      },
      "source": [
        "## (2) vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2EvjONm1bJX",
        "outputId": "729f0a44-2730-48b1-9263-2185af33b36d"
      },
      "source": [
        "def text2sequence(train_text, max_len=1000):\n",
        "    \n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(train_text)\n",
        "    train_X_seq = tokenizer.texts_to_sequences(train_text)\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    print('vocab_size : ', vocab_size)\n",
        "    X_train = pad_sequences(train_X_seq, maxlen = max_len)\n",
        "    return X_train, vocab_size, tokenizer\n",
        "\n",
        "train_y = train['author']\n",
        "train_X, vocab_size, vectorizer = text2sequence(train['token'], max_len = 100)\n",
        "print(train_X.shape, train_y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size :  36342\n",
            "(54879, 100) (54879,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glQSIN5I1gfS"
      },
      "source": [
        "## (3) Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPaJtvH91jRz"
      },
      "source": [
        "word2vec = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/ESAA-2/GoogleNews-vectors-negative300.bin.gz', binary = True)\n",
        "embedding_matrix = np.zeros((vocab_size, 300))\n",
        "\n",
        "for index, word in enumerate(vocabulary):\n",
        "    if word in word2vec:\n",
        "        embedding_vector = word2vec[word] \n",
        "        embedding_mxtrix[i] = embedding_vector \n",
        "    else:\n",
        "        print(\"word2vec에 없는 단어입니다.\")\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Osf8wPfC1l2g"
      },
      "source": [
        "## (4) Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiRW2gwd1oMD"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Embedding\n",
        "\n",
        "def LSTM(vocab_size, max_len=1000):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, 300,weights = [embedding_matrx], input_length = max_len)) #임베딩 가중치 적용 코드\n",
        "    model.add(SpatialDropout1D(0.3))\n",
        "    model.add(LSTM(64))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(64, activation='relu', kernel_regularizer = regularizers.l2(0.001)))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics='accuracy')\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}